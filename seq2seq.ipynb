{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1W2Mlb_PCqfKnEY7aOvLMI5uE9TbRpK2L",
      "authorship_tag": "ABX9TyPnRbQpKWjQAXWHQ6axXPVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravit-cohen-segev/cats/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AguObDHjqgU"
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBeHsdpunGy8"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wA_gENBjs2b"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZZdk5yqn-It"
      },
      "source": [
        "\n",
        "downloaded = drive.CreateFile({'id':\"seq2seq_GAN\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWXhdON1xyIr"
      },
      "source": [
        "#install rdkit (for reading smiles from mol file)\n",
        "!pip install kora\n",
        "import kora.install.rdkit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ_3V-izo9RE"
      },
      "source": [
        "!pip install sklearn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXnCVjszokGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58b4be3a-4d80-45bd-d6ed-86c4321d7911"
      },
      "source": [
        "import rdkit.Chem\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LeakyReLU, Add, Concatenate, Activation, Dense, BatchNormalization, LSTM, Input, Concatenate, LeakyReLU\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras import Model, Sequential, losses, initializers, regularizers\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.callbacks import History\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from joblib import load, dump\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "smiles_vocab = [' ',\n",
        "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
        "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                '=', '@',\n",
        "                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
        "                'R', 'S', 'T', 'V', 'X', 'Z',\n",
        "                '[', '\\\\', ']',\n",
        "                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
        "                't', 'u', 'None']\n",
        "max_len = 100\n",
        "latent_dim = 64\n",
        "len_vocab=57\n",
        "\n",
        "#create new generated molecules\n",
        "class Generator:\n",
        "    def __init__(self, max_len=100, len_vocab=57):\n",
        "        self.max_len = max_len\n",
        "        self.len_vocab = len_vocab\n",
        "\n",
        "    #create fake onehots with rules: smiles must start with a letter. closing brackets will not appear before opening brackets\n",
        "    def create_onehot_mat_random(self, sample_num):\n",
        "        #list of characters to exclude if there isn't an opening bracket of if another type of bracket is already open\n",
        "        exclude_ch = ['(', ')', '[', ']']\n",
        "\n",
        "        onehot_mat = np.zeros((sample_num, self.max_len, self.len_vocab))\n",
        "        minimum_size_mol = 20\n",
        "        min_end_chain = 1\n",
        "        max_end_chain = self.max_len - minimum_size_mol\n",
        "\n",
        "        # create array of indices that point only to letters in smiles_vocab. This is for the first letter of smile\n",
        "        first_lett_1 = np.arange(21, 39)\n",
        "        first_lett_2 = np.arange(42, 56)\n",
        "        first_lett_comb = np.append(first_lett_1, first_lett_2)\n",
        "\n",
        "        for i in range(sample_num):\n",
        "            random_end_len = np.random.randint(min_end_chain, max_end_chain)\n",
        "            #first random letter\n",
        "            r1 = np.random.choice(first_lett_comb,1)\n",
        "            onehot_mat[i, 0] = np.eye(self.len_vocab)[r1]\n",
        "\n",
        "            open_brackets_count = 0\n",
        "            open_square_brackets_count = 0\n",
        "\n",
        "            for j in range(1, max_len - random_end_len - 1): #start from second character\n",
        "                allowed_ch = np.arange(self.len_vocab)\n",
        "\n",
        "                # can't use close brackets if there isn't an open brackets before\n",
        "                if open_brackets_count == 0:\n",
        "                    allowed_ch = np.setdiff1d(allowed_ch, 4)\n",
        "                else:\n",
        "                    allowed_ch = np.setdiff1d(allowed_ch, (39, 41))  # make only one type of brackets is open\n",
        "\n",
        "                if open_square_brackets_count == 0:\n",
        "                    allowed_ch = np.setdiff1d(allowed_ch, 41)\n",
        "                else:\n",
        "                    allowed_ch = np.setdiff1d(allowed_ch, (3, 4))  # make only one type of brackets is open\n",
        "\n",
        "                r = np.random.choice(allowed_ch)\n",
        "                # if character is already a bracket, continue searching for a position that is not a bracket\n",
        "                while smiles_vocab[np.argmax(onehot_mat[i, j])] in exclude_ch:\n",
        "                    r = np.random.choice(allowed_ch)\n",
        "\n",
        "                onehot_mat[i, j, :] = np.eye(self.len_vocab)[r]\n",
        "\n",
        "                if smiles_vocab[r] == ')':\n",
        "                    open_brackets_count -= 1\n",
        "\n",
        "                if smiles_vocab[r] == ']':\n",
        "                    open_square_brackets_count -=1 \n",
        "                if smiles_vocab[r] == '(':\n",
        "                    open_brackets_count += 1\n",
        "                    o_bracket_p0 = j\n",
        "\n",
        "                if smiles_vocab[r] == '[':\n",
        "                    open_square_brackets_count += 1\n",
        "\n",
        "        new_end = random_end_len\n",
        "\n",
        "        for col in range(self.max_len - new_end, self.max_len):\n",
        "            onehot_mat[i, col, -1] = 1\n",
        "\n",
        "        return onehot_mat\n",
        "\n",
        "#create autoencoder\n",
        "class Decoder_seq2seq_VAE:\n",
        "    def __init__(self, max_len=100):\n",
        "        self.max_len = max_len\n",
        "        self.vocab = smiles_vocab\n",
        "        # for adding a positional feature\n",
        "        self.smi2index = dict((c, i) for i, c in enumerate(self.vocab))\n",
        "\n",
        "    def smiles_to_onehot_mat(self, smiles):\n",
        "        len_vocab = len(self.vocab)\n",
        "        onehot_mat = []\n",
        "        for smile in smiles:\n",
        "            m = rdkit.Chem.MolFromSmiles(smile, sanitize=False)\n",
        "            if m is None:\n",
        "                onehot_mat.append(np.nan)\n",
        "            else:\n",
        "                smile = rdkit.Chem.MolToSmiles(m)\n",
        "                x = np.zeros((self.max_len, len_vocab))\n",
        "\n",
        "                c_i = 0\n",
        "                for idx, c in enumerate(smile):\n",
        "                    x[idx, self.smi2index[c]] = 1\n",
        "                    c_i = idx\n",
        "                # if smile is smaller than max_len than the remaining indices of chars will be equal to the index of none in vocab\n",
        "                while c_i < (self.max_len - 1):\n",
        "                    c_i += 1\n",
        "                    x[idx, self.smi2index['None']] = 1\n",
        "                onehot_mat.append(x)\n",
        "        #return np.asarray(onehot_mat, dtype=np.int8)\n",
        "        return np.asarray(onehot_mat, dtype=np.float16)\n",
        "\n",
        "    def smiles_decoder(self, output):\n",
        "        index2smi = dict((i, c) for i, c in enumerate(self.vocab))\n",
        "        decoded_smiles = []\n",
        "        for row in range(len(output)):\n",
        "            smi = ''\n",
        "            for i in range(len(output[row])):\n",
        "                atom_idx = output[row, i, :].argmax(axis=-1)\n",
        "                if index2smi[atom_idx]!='None':\n",
        "                   smi = smi + index2smi[atom_idx]\n",
        "                else:\n",
        "                    break\n",
        "            decoded_smiles.append(smi)\n",
        "        return decoded_smiles\n",
        "\n",
        "    def build_encoder_decoder_model(self, max_len, input_shape, latent_dim, optimization):\n",
        "\n",
        "        lstm_units = 64\n",
        "        kernel_regularizer = regularizers.l2(1e-5)\n",
        "\n",
        "        encoder_input = Input(shape=input_shape, name='Input_encoder')\n",
        "        lstm_encoder = LSTM(lstm_units, return_sequences=True, activation='relu', name='lstm_encoder')(encoder_input)\n",
        "        encoder_output = Dense(latent_dim, activation='relu', name='Dense_encoder')(lstm_encoder)\n",
        "\n",
        "        encoder = Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "\n",
        "        decoder_input = Input(shape=(max_len, latent_dim), name='decoder_input')\n",
        "        decoder_batch = BatchNormalization()(decoder_input)\n",
        "        decoder_output = LSTM(len(smiles_vocab), kernel_regularizer=kernel_regularizer, return_sequences=True,\n",
        "                            activation='softmax', name='lstm_decoder')(decoder_batch)\n",
        "        decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "        x = encoder(encoder_input)\n",
        "        output = decoder(x)\n",
        "\n",
        "        autoencoder_model = Model(encoder_input, output, name=\"autoencoder\")\n",
        "        loss_fn = losses.CategoricalCrossentropy()\n",
        "\n",
        "\n",
        "        autoencoder_model.compile(loss=loss_fn, optimizer=optimization,\n",
        "                                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "        return autoencoder_model\n",
        "\n",
        "    def train_autoencoder(self, files_per_iter, end_file, batches, epochs, latent_dim, optimization) -> object:\n",
        "        m = True\n",
        "        for epoch in range(epochs):\n",
        "           print(\"epoch num is:\", epoch)\n",
        "           i = 0\n",
        "           while i<end_file:\n",
        "              y_train, y_train_onehot = prepare_data(i, i+files_per_iter)\n",
        "              new_dim = y_train_onehot.shape\n",
        "              input_shape = (new_dim[1], new_dim[2])\n",
        "              if m:\n",
        "                 autoencoder = self.build_encoder_decoder_model(max_len, input_shape, latent_dim, optimization)\n",
        "                 m = False\n",
        "              i += files_per_iter\n",
        "              autoencoder.fit(y_train_onehot, y_train_onehot, batch_size = batches)\n",
        "        serialize_to_json(autoencoder, 'autoencoder')\n",
        "        return autoencoder\n",
        "    \n",
        "class Discriminator:\n",
        "    def __init__(self, max_len=100, len_vocab=57):\n",
        "        self.max_len = max_len\n",
        "        self.len_vocab = len_vocab\n",
        "\n",
        "    \n",
        "\n",
        "    def compile_discriminator(self, optimizer):\n",
        "        '''json_file = open(\"autoencoder.json\", 'r')\n",
        "        loaded_model = json_file.read()\n",
        "        json_file.close()\n",
        "        autoencoder = model_from_json(loaded_model)\n",
        "        autoencoder.load_weights(\"autoencoder.h5\")\n",
        "        #set_weights from last layer of autoencoder\n",
        "        #w = autoencoder.layers[-1].get_weights()'''\n",
        "        input_shape = (self.max_len, self.len_vocab)\n",
        "        #discriminator_model = Sequential()\n",
        "        #discriminator_model.add(Input(shape=input_shape))\n",
        "        kernel_regularizer = regularizers.l2(1e-5)\n",
        "\n",
        "        input_D = Input(shape=input_shape)\n",
        "        lstm1_D = LSTM(16, kernel_initializer=initializers.RandomNormal(), kernel_regularizer=kernel_regularizer,\n",
        "                       return_sequences=True, activation='relu')(input_D)\n",
        "\n",
        "        lstm2_D = LSTM(16, kernel_initializer=initializers.RandomNormal(), kernel_regularizer=kernel_regularizer,\n",
        "                       activation='relu')(lstm1_D)\n",
        "        batch = BatchNormalization()(lstm2_D)\n",
        "        output_D = Dense(1, kernel_regularizer=kernel_regularizer,\n",
        "                         activation='sigmoid')(batch)\n",
        "        discriminator_model = Model(inputs=input_D, outputs=output_D, name=\"Discriminator\")\n",
        "        discriminator_model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "        return discriminator_model\n",
        "\n",
        "    def train_discriminator(self, t_data, labels, batches, epochs, optimizer=SGD(0.00001)):\n",
        "        model = self.compile_discriminator(optimizer)\n",
        "        model.fit(t_data, labels, batch_size=batches, epochs=epochs)\n",
        "        serialize_to_json(model, 'Discriminator')\n",
        "        return model\n",
        "\n",
        "    def create_data_for_discriminator(self, file_ix):\n",
        "        y_arr, y_onehot = load_data(file_ix, max_len)\n",
        "        labels_true = np.ones((len(y_onehot),1))\n",
        "\n",
        "        #create fake data\n",
        "        gen = Generator()\n",
        "        y_fake = gen.create_onehot_mat_random(len(y_onehot))\n",
        "        labels_false = np.zeros((len(y_onehot),1))\n",
        "\n",
        "        #combine to one dataset\n",
        "        data = np.concatenate((y_onehot, y_fake))\n",
        "        labels =  np.concatenate((labels_true, labels_false))\n",
        "        #shuffle rows\n",
        "        shuffled_data, shuffled_labels = shuffle(data, labels, random_state=0)\n",
        "        return shuffled_data, shuffled_labels\n",
        "\n",
        "\n",
        "class ZincGAN():\n",
        "    def __init__(self):\n",
        "        self.Autoencoder = load_model_from_file(\"autoencoder\")\n",
        "\n",
        "        self.Discriminator = Discriminator().compile_discriminator(optimizer=SGD(0.001))\n",
        "        self.l = 21184\n",
        "        self.len_vocab = len_vocab\n",
        "\n",
        "    def config_model(self, model, target_model):\n",
        "        weights = model.get_weights()\n",
        "        target_model.set_weights(weights)\n",
        "        return\n",
        "\n",
        "    def custom_generator_loss(self, y_true, y_pred):\n",
        "        #  Disc_pred = self.Discriminator.predict(y_pred)\n",
        "        D_mse_loss = K.sqrt(K.square(y_pred - y_true))\n",
        "        loss = K.log(1 - D_mse_loss)\n",
        "        return loss\n",
        "\n",
        "    def build_combined_gan(self, optimizer=SGD(0.001)):\n",
        "        # make weights in the discriminator not trainable\n",
        "        self.Discriminator.trainable = False\n",
        "        self.combined = Sequential()\n",
        "        # combine the models\n",
        "\n",
        "        inputs = self.Autoencoder.input\n",
        "        inter = self.Autoencoder(inputs)\n",
        "        outputs = self.Discriminator(inter)\n",
        "\n",
        "        #    input = self.Autoencoder(inputs)\n",
        "        #   out = self.Discriminator(input)\n",
        "\n",
        "        self.combined = Model(inputs=inputs, outputs=outputs, name=\"gan_model\")\n",
        "        # compile\n",
        "        self.combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.combined\n",
        "\n",
        "    def train_GAN(self, latent_dim=57, n_batch=128, epochs=500, total_files=20):\n",
        "        steps_per_epoch = int(self.l / n_batch)\n",
        "\n",
        "        history_loss_g = []\n",
        "        history_acc_g = []\n",
        "        history_loss_d = []\n",
        "        history_acc_d = []\n",
        "        history = History()\n",
        "        # compile combine model\n",
        "        self.build_combined_gan()\n",
        "        #config autoencoder weights with gan nn\n",
        "\n",
        "        self.config_model(self.Autoencoder, self.combined.layers[1])\n",
        "        for epoch in range(epochs):\n",
        "            print(\"{} epoch/ {} epochs\".format(epoch, epochs))\n",
        "            #        for j in range(batches):\n",
        "            gc.collect()\n",
        "            # print(\"{} batch/ {} batches\".format(j, batches))\n",
        "            # create fake onehots\n",
        "            fake_data = create_fake_onehots(self.l)\n",
        "            generated_data = self.Autoencoder.predict(fake_data)\n",
        "            labels_false = np.zeros((self.l, 1))\n",
        "\n",
        "            # choose random molecules from dataset (real samples)\n",
        "            file_ix = np.random.randint(total_files)\n",
        "            y_arr, real_data = load_data(file_ix, max_len)\n",
        "            labels_true = np.ones((self.l, 1))\n",
        "\n",
        "            x_generator = np.vstack((real_data, generated_data))\n",
        "            y_generator = np.vstack((labels_true, labels_false))\n",
        "            D_generator = [x_generator, y_generator]\n",
        "\n",
        "            # x_validation = np.vstack((real_data[20000:], fake_data[20000:]))\n",
        "            # y_validation = np.vstack((labels_true[20000:], labels_false[20000:]))\n",
        "            #            D_validation = [x_validation,y_validation]\n",
        "\n",
        "            #       D_partition = {'train':x_generator, 'test':x_validation}\n",
        "\n",
        "            # update discriminator model weights\n",
        "            print(\"Train Discriminator\")\n",
        "            #     before = self.Discriminator.predict(x_generator)\n",
        "            h_d = self.Discriminator.fit(x_generator, y_generator, batch_size=steps_per_epoch, epochs=1,\n",
        "                                         callbacks=[history])\n",
        "            #    after = self.Discriminator.predict(x_generator)\n",
        "            #   merged = np.hstack((before, after, y_generator))\n",
        "            # configure Discriminator weights between both models\n",
        "            self.config_model(self.Discriminator, self.combined.layers[-1])\n",
        "\n",
        "            # create random onehots for gan\n",
        "            x_gan = create_fake_onehots(self.l * 2)\n",
        "            # create inverse labels for the fake data\n",
        "            y_gan = np.ones((self.l * 2, 1))\n",
        "            # update the generator via the discriminator error\n",
        "\n",
        "            gan_generator = [x_gan, y_gan]\n",
        "            #       gan_validation = [x_gan[20000:], y_gan[20000:]]\n",
        "\n",
        "            #       G_partition = {'train':gan_generator, 'test':gan_validation}\n",
        "            print(\"Train Gan\")\n",
        "\n",
        "            h_g = self.combined.fit(x_gan, y_gan, batch_size=steps_per_epoch, epochs=1, callbacks=[history])\n",
        "            #configure autoencoder weights\n",
        "            self.config_model(self.combined.layers[1], self.Autoencoder)\n",
        "\n",
        "            history_loss_d.append(h_d.history['loss'])\n",
        "            history_acc_d.append(h_d.history['accuracy'])\n",
        "            history_loss_g.append(h_g.history['loss'])\n",
        "            history_acc_g.append(h_g.history['accuracy'])\n",
        "            cutoff = 0.9\n",
        "            print(\"END of epoch:\", epoch)\n",
        "            \n",
        "            #save generator model every 10 epochs\n",
        "            if epoch%10 == 0:\n",
        "               serialize_to_json(self.Autoencoder, 'generator')\n",
        "        serialize_to_json(self.combined, 'gan')\n",
        "        return self.combined, self.Discriminator, history_loss_g, history_acc_g, history_loss_d, history_acc_d\n",
        "\n",
        "#functions outside classes\n",
        "def generator_predict(self, sample_num):\n",
        "   json_file = open(\"autoencoder.json\", 'r')\n",
        "   loaded_model = json_file.read()\n",
        "   json_file.close()\n",
        "   autoencoder = model_from_json(loaded_model)\n",
        "   autoencoder.load_weights(\"autoencoder.h5\")\n",
        "   onehot_input = self.create_onehot_mat_random(sample_num)\n",
        "   pred = autoencoder.predict(onehot_input)\n",
        "   dec = Decoder_seq2seq_VAE()\n",
        "   onehot = dec.smiles_decoder(pred)\n",
        "   return onehot\n",
        "\n",
        "#load saved data\n",
        "def prepare_data(start, end):\n",
        "   for i in range(start, end):\n",
        "      temp_y, temp_onehot = load_data(i, max_len)\n",
        "      if i==start:\n",
        "         y_arr = temp_y\n",
        "         y_onehot = temp_onehot\n",
        "      else:\n",
        "         y_arr = np.concatenate((y_arr, temp_y), axis=0)\n",
        "         y_onehot = np.concatenate((y_onehot, temp_onehot), axis=0)\n",
        "   return y_arr, y_onehot\n",
        "\n",
        "#save weights \n",
        "def weights_to_file(model):\n",
        "    g_w = {}\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        g_w['w' + str(i)] = layer.get_weights()\n",
        "\n",
        "    with open('weights.txt', 'w') as file:\n",
        "        for item in g_w.keys():\n",
        "            file.write(str(g_w[item]))\n",
        "    return\n",
        "\n",
        "#print predictions to file\n",
        "def pred_to_file(y_pred, type_label):\n",
        "    with open(type_label + '.txt', 'w') as file:\n",
        "        for val in y_pred:\n",
        "            file.write(str(val) + '\\n')\n",
        "    return\n",
        "\n",
        "\n",
        "def load_file(file_name):\n",
        "    temp = np.load(r\"C:\\Users\\ravit\\PycharmProjects\\Barak_BGU\\data\\data_for_keras0.npy\", allow_pickle=True)\n",
        "    return temp\n",
        "\n",
        "def load_data(i, max_len):\n",
        "    path = r\"/content/drive/My Drive/pycharm/seq2seq_GAN/data/\"\n",
        "    label_file = path + \"data_for_keras\" + \"label\" + str(i) + \".txt\"\n",
        "\n",
        "    with open(label_file) as label:\n",
        "        y_arr = label.readlines()\n",
        "        smi = Decoder_seq2seq_VAE()\n",
        "        y_onehot = smi.smiles_to_onehot_mat(y_arr)\n",
        "    return y_arr, y_onehot\n",
        "\n",
        "#save model layers and weights in json files\n",
        "def serialize_to_json(model, name):\n",
        "    model_json = model.to_json()\n",
        "    with open(name+\".json\", 'w') as json_file:\n",
        "        json_file.write(model_json)\n",
        "    model.save_weights(name+\".h5\")\n",
        "    return\n",
        "\n",
        "\n",
        "def load_model_from_file(model_name):\n",
        "    json_file = open(r\"/content/drive/My Drive/pycharm/seq2seq_GAN/\" + model_name + \".json\", 'r')\n",
        "    loaded_model = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model)\n",
        "    model.load_weights(r\"/content/drive/My Drive/pycharm/seq2seq_GAN/\" + model_name + \".h5\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_fake_onehots(l):\n",
        "    gen = Generator()\n",
        "    y_fake = gen.create_onehot_mat_random(l)\n",
        "    return y_fake\n",
        "\n",
        "\n",
        "def generator_input_output(l, gan_model):\n",
        "    generator = Sequential()\n",
        "    generator.add(gan_model.layers[0])\n",
        "    x = create_fake_onehots(l)\n",
        "    y_pred = generator.predict(x)\n",
        "    dec = Decoder_seq2seq_VAE()\n",
        "    smi_res = dec.smiles_decoder(y_pred)\n",
        "    #print results\n",
        "    print(smi_res)\n",
        "    \n",
        "    #save to file\n",
        "    for i, line in enumerate(smi_res):\n",
        "      with open(epochs+\"epochs_generated_mols.txt\", 'w') as f:\n",
        "         f.writelines(smi_res)\n",
        "         f.close()\n",
        "    return \n",
        "\n",
        "def plot_history(le, h_loss_g, h_acc_g, h_loss_d, h_acc_d):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    ax1.plot(np.arange(le), history_acc_g, 'r')\n",
        "    ax1.plot(np.arange(le), history_acc_d, 'g')\n",
        "    ax1.set_title('accuracy_over_time')\n",
        "    ax1.set_ylabel('accuracy')\n",
        "    ax1.set_xlabel('epochs')\n",
        "\n",
        "    # summarize history for loss\n",
        "\n",
        "    ax2.plot(np.arange(200), history_loss_g, 'r')\n",
        "    ax2.plot(np.arange(200), history_loss_d, 'g')\n",
        "    ax2.set_title('loss_over_time')\n",
        "    ax2.set_ylabel('loss')\n",
        "    ax2.set_xlabel('epochs')\n",
        "\n",
        "    plt.ylim(0, 5)\n",
        "    fig.show()\n",
        "    return\n",
        "\n",
        "def predict_autoencoder(n, epochs, discriminator_model):\n",
        "    autoencoder = load_model_from_file('generator')\n",
        "    a_data = create_fake_onehots(n)\n",
        "    gen_pred = autoencoder.predict(a_data)\n",
        "    gen = Decoder_seq2seq_VAE()\n",
        "    decoded = gen.smiles_decoder(gen_pred)\n",
        "    y_disc = discriminator_model.predict(gen_pred)\n",
        "    pred_to_file(y_disc, str(epochs) + 'num_epochs')\n",
        "    return decoded, y_disc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # check GPU available\n",
        "\n",
        "    print('Version of tensorflow:\\n', tf.__version__)\n",
        "    print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "        device_name = tf.test.gpu_device_name()\n",
        "    else:\n",
        "        device_name = '/CPU:0'\n",
        "    print(device_name)\n",
        "\n",
        "    epochs = 150\n",
        "    gan = ZincGAN()\n",
        "    gan_model, discriminator_model, history_loss_g, history_acc_g, history_loss_d, history_acc_d \\\n",
        "        = gan.train_GAN(n_batch=128, epochs=epochs, total_files=20)\n",
        "\n",
        "    plot_history(epochs, history_loss_g, history_acc_g, history_loss_d, history_acc_d)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version of tensorflow:\n",
            " 2.3.0\n",
            "GPU Available: []\n",
            "/CPU:0\n",
            "0 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 11s - loss: 0.2503 - accuracy: 0.1212WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0841s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0841s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 19s 73ms/step - loss: 0.2495 - accuracy: 0.9012\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 32s - loss: 0.6933 - accuracy: 0.6182WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.2511s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.2511s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 61s 237ms/step - loss: 0.6933 - accuracy: 0.6512\n",
            "END of epoch: 0\n",
            "1 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2498 - accuracy: 0.8727WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0705s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0705s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 72ms/step - loss: 0.2498 - accuracy: 0.8897\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 29s - loss: 0.6937 - accuracy: 0.4182WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.2256s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.2256s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 235ms/step - loss: 0.6935 - accuracy: 0.5181\n",
            "END of epoch: 1\n",
            "2 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2498 - accuracy: 0.8667WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0643s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0643s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 70ms/step - loss: 0.2499 - accuracy: 0.9101\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 28s - loss: 0.6937 - accuracy: 0.5152WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.2126s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.2126s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 234ms/step - loss: 0.6934 - accuracy: 0.6326\n",
            "END of epoch: 2\n",
            "3 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2499 - accuracy: 0.9758WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0695s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0695s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 69ms/step - loss: 0.2499 - accuracy: 0.7910\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 28s - loss: 0.6937 - accuracy: 0.0515WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.2073s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.2073s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 235ms/step - loss: 0.6936 - accuracy: 0.1198\n",
            "END of epoch: 3\n",
            "4 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2499 - accuracy: 0.9909WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0715s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0715s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 71ms/step - loss: 0.2499 - accuracy: 0.9795\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 28s - loss: 0.6938 - accuracy: 0.0515WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.2151s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.2151s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 232ms/step - loss: 0.6936 - accuracy: 0.0600\n",
            "END of epoch: 4\n",
            "5 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2500 - accuracy: 0.9818WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0700s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0700s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 70ms/step - loss: 0.2500 - accuracy: 0.8373\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 29s - loss: 0.6937 - accuracy: 0.0273WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.2178s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.2178s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 232ms/step - loss: 0.6937 - accuracy: 0.0305\n",
            "END of epoch: 5\n",
            "6 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 8s - loss: 0.2500 - accuracy: 0.9788WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0674s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0674s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 68ms/step - loss: 0.2500 - accuracy: 0.9624\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 31s - loss: 0.6938 - accuracy: 0.0091WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0215s vs `on_train_batch_end` time: 0.2206s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0215s vs `on_train_batch_end` time: 0.2206s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 234ms/step - loss: 0.6939 - accuracy: 0.0043\n",
            "END of epoch: 6\n",
            "7 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2500 - accuracy: 0.9788WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0659s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0659s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 17s 66ms/step - loss: 0.2500 - accuracy: 0.7977\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 28s - loss: 0.7056 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.2172s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.2172s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 232ms/step - loss: 0.7056 - accuracy: 0.0000e+00\n",
            "END of epoch: 7\n",
            "8 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 8s - loss: 0.2500 - accuracy: 0.9848WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0547s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0547s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 71ms/step - loss: 0.2500 - accuracy: 0.9823\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 29s - loss: 0.6956 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.2232s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.2232s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 59s 231ms/step - loss: 0.6955 - accuracy: 0.0000e+00\n",
            "END of epoch: 8\n",
            "9 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2500 - accuracy: 0.9909WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0667s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0667s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 70ms/step - loss: 0.2500 - accuracy: 0.8923\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 29s - loss: 0.6939 - accuracy: 0.0030WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.2196s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.2196s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 232ms/step - loss: 0.6939 - accuracy: 0.0052\n",
            "END of epoch: 9\n",
            "10 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 8s - loss: 0.2500 - accuracy: 0.9848WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0634s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0634s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 69ms/step - loss: 0.2500 - accuracy: 0.7469\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 28s - loss: 0.6934 - accuracy: 0.2424WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.2164s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.2164s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 59s 231ms/step - loss: 0.6934 - accuracy: 0.3070\n",
            "END of epoch: 10\n",
            "11 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2500 - accuracy: 0.9788WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0629s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0629s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 70ms/step - loss: 0.2500 - accuracy: 0.7413\n",
            "Train Gan\n",
            "  2/257 [..............................] - ETA: 30s - loss: 0.6935 - accuracy: 0.0485WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.2290s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_train_batch_end` time: 0.2290s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 60s 232ms/step - loss: 0.6935 - accuracy: 0.1410\n",
            "END of epoch: 11\n",
            "12 epoch/ 150 epochs\n",
            "Train Discriminator\n",
            "  2/257 [..............................] - ETA: 9s - loss: 0.2500 - accuracy: 0.9909WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0052s vs `on_train_batch_end` time: 0.0698s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0052s vs `on_train_batch_end` time: 0.0698s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257/257 [==============================] - 18s 70ms/step - loss: 0.2500 - accuracy: 0.9629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZmpwVFu1-Vv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7nKakbyEks"
      },
      "source": [
        "generator = Sequential()\n",
        "generator.add(gan_model.layers[0])\n",
        "x = create_fake_onehots(1000)\n",
        "y_pred = generator.predict(x)\n",
        "dec = Decoder_seq2seq_VAE()\n",
        "smi_res = dec.smiles_decoder(y_pred)\n",
        "with open(str(epochs)+\"epochs_generated_mols.txt\", 'w') as f:\n",
        "   print(str(smi_res), file=f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTmLEBW8v1Nu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "6f5377a0-fd6e-4025-db2d-f4fe58177192"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "# summarize history for accuracy\n",
        "ax1.plot(np.arange(epochs), history_acc_g, 'r')\n",
        "ax1.plot(np.arange(epochs), history_acc_d, 'g')\n",
        "ax1.set_title('accuracy_over_time')\n",
        "ax1.set_ylabel('accuracy')\n",
        "ax1.set_xlabel('epochs')\n",
        "\n",
        "# summarize history for loss\n",
        "\n",
        "ax2.plot(np.arange(epochs), history_loss_g, 'r')\n",
        "ax2.plot(np.arange(epochs), history_loss_d, 'g')\n",
        "ax2.set_title('loss_over_time')\n",
        "ax2.set_ylabel('loss')\n",
        "ax2.set_xlabel('epochs')\n",
        "\n",
        "plt.ylim(0, 5)\n",
        "fig.show()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}